{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB Sentiment Analysis with Bi-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrfLlHg6FnY3fylufDGwqw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62qTog4M6Pml",
        "colab_type": "text"
      },
      "source": [
        "## Import of librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl2gZl2RFNUL",
        "colab_type": "code",
        "outputId": "bf4712dd-fa7f-4c68-ecd2-3c0e3ebfa0f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras import *\n",
        "from keras.layers import Embedding, Dense, Dropout, Bidirectional\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ou7xgN16bdo",
        "colab_type": "text"
      },
      "source": [
        "## Loading of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD56r0ZWCUek",
        "colab_type": "text"
      },
      "source": [
        "link : https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8BZdNzhENqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KauvNu-_EkfR",
        "colab_type": "code",
        "outputId": "d2c96e95-5309-4867-fe17-5920a43cbc18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IKEW9QYMvMD",
        "colab_type": "text"
      },
      "source": [
        "Distribution of positive and negative sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vn5lALIG0Ca",
        "colab_type": "code",
        "outputId": "7c4cc607-a4d0-46a0-e9ce-9590ea06f339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    25000\n",
              "negative    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8oW3gIi6vpX",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYnzgxLoMKGW",
        "colab_type": "text"
      },
      "source": [
        "Replacement of values of target column by numrical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSBEdOUHFIQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.sentiment.replace(['positive', 'negative'], [1, 0], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTTuyP76oIWF",
        "colab_type": "text"
      },
      "source": [
        "Building of a list for reviews and another for sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU5TX2cKNKlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = [review for review in df['review']]\n",
        "sentiments = [sentiment for sentiment in df['sentiment'] ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npkmY-kUAaVF",
        "colab_type": "text"
      },
      "source": [
        "Function to remove useless information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV6vj_CPrfDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_preprocessing (text, min_word_length) :\n",
        "  \"\"\"\n",
        "\n",
        "  text : must be a string\n",
        "  min_word_length : must be a integer \n",
        "\n",
        "  function return a string\n",
        "\n",
        "  \"\"\"\n",
        "  # put in lowercase all words\n",
        "  text = text.lower()\n",
        "  # remove html tags\n",
        "  text = text.replace('{html}',\"\")\n",
        "  # remove URL\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  # remove numbers\n",
        "  text = re.sub('[0-9]+', '', text)\n",
        "  # tokenize the text and convert string to list\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  text = tokenizer.tokenize(text)\n",
        "  # removing of stopwords\n",
        "  text = [word for word in text if len(word) > min_word_length if word not in stopwords.words('english')]\n",
        "  # Lemmatization\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  text=[lemmatizer.lemmatize(word) for word in text]\n",
        "  # convert list to string\n",
        "  text = \" \".join(text)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-dF3F9lAjVH",
        "colab_type": "text"
      },
      "source": [
        "Applying of function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk_ZrBZCr92H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = [text_preprocessing (review, 1) for review in reviews]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz9l-IuuA8qC",
        "colab_type": "text"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnVg68ACFcmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(\n",
        "    num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,\n",
        "    split=' ', char_level=False, oov_token=None, document_count=0)\n",
        "\n",
        "# The filters remove useless character for a text analysis\n",
        "#lower = True puts each word in lowercase"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_vJ7GSvBDQw",
        "colab_type": "text"
      },
      "source": [
        "Get the vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL2cnDh3yzyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying of tokenizer function on all texts\n",
        "\n",
        "tokenizer.fit_on_texts(reviews)\n",
        "\n",
        "# Get dictionnary with a word as key and the index as value\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "needfRL0KGfq",
        "colab_type": "code",
        "outputId": "3a9922f4-b806-4be0-beac-e02d9bb00300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print ('Vocabulary size: ' + str(len(word_index)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 90198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG8tnRCyBLx3",
        "colab_type": "text"
      },
      "source": [
        "Convert to sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DldnS3D82ACX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCcYki7LJIGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded_sequences = pad_sequences(sequences, padding='post', maxlen= 150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO3mPYLVLsZd",
        "colab_type": "code",
        "outputId": "419b3323-56a4-4b3f-b7e7-2b9f03eb62d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get the maximum length of all sequences\n",
        "\n",
        "maxlen = 0\n",
        "\n",
        "for sequence in sequences :\n",
        "  if len(sequence)> maxlen :\n",
        "    maxlen = len(sequence)\n",
        "\n",
        "print('maximum length: ' + str(maxlen))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum length: 1427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxMzKTfhBScC",
        "colab_type": "text"
      },
      "source": [
        "## Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeHW9Ua9N0zV",
        "colab_type": "code",
        "outputId": "c1a0cd6f-dc2b-459e-e32c-5ed4ade10091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = len(word_index)+1\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=150))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 150, 100)          9019900   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 9,104,509\n",
            "Trainable params: 9,104,509\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWnB9P6vRIoy",
        "colab_type": "code",
        "outputId": "45dec957-a898-4327-bc93-b195348e8079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "epochs = 3\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-5,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.9, name='Adam')\n",
        "\n",
        "\n",
        "model.compile(optimizer='Adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    padded_sequences, sentiments,\n",
        "    epochs=epochs, validation_split = 0.2, batch_size = 200)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "40000/40000 [==============================] - 135s 3ms/step - loss: 0.5422 - accuracy: 0.9143 - val_loss: 0.5697 - val_accuracy: 0.8491\n",
            "Epoch 2/3\n",
            "40000/40000 [==============================] - 132s 3ms/step - loss: 0.5392 - accuracy: 0.9219 - val_loss: 0.5636 - val_accuracy: 0.8757\n",
            "Epoch 3/3\n",
            "40000/40000 [==============================] - 130s 3ms/step - loss: 0.5333 - accuracy: 0.9366 - val_loss: 0.5634 - val_accuracy: 0.8710\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}